% !TEX program = xelatex
% !TEX pweaveOutputFormat = tex
% cSpell: disable

\documentclass{report}
\usepackage{arxiv}
\renewcommand{\arraystretch}{1.25}

\usepackage{multirow}
\usepackage{amssymb,mathtools}
\usepackage{booktabs}
\usepackage{verbatim}

\usepackage{hyperref}
\hypersetup
{ pdfauthor = {Gyan Sinha},
  pdftitle={Loan Payment Distress Due to COVID-19: A Case Study},
  colorlinks=TRUE,
  linkcolor=black,
  citecolor=blue,
  urlcolor=blue
}
%
\RequirePackage{fontspec}
\setmainfont{Source Sans Pro}
\usepackage{graphicx}
\graphicspath{/home/gsinha/admin/docs/logos}

\setcounter{tocdepth}{3}
\setcounter{secnumdepth}{3}

<<imports, echo=False>>=
import warnings
warnings.filterwarnings("ignore")
import sys
import datetime

import matplotlib.pyplot as plt
import matplotlib.ticker as mtick
import seaborn as sns
sns.set()

plt.rcParams.update({
    "font.family": "Source Sans Pro",
    "font.serif": ["Source Sans Pro"], 
    "font.sans-serif": ["Source Sans Pro"],
    "font.size": 10,
})

import pathlib
import joblib
import numpy as np
import pandas as pd
import geopandas as gpd
import pytoml

from fredapi import Fred

import feather

import pymc3 as pm
import arviz as az

import lifelines
from lifelines import KaplanMeierFitter, NelsonAalenFitter
from scipy.special import expit

from analytics import utils

models_dir = "/home/gsinha/admin/db/dev/Python/projects/models/"
import_dir = models_dir + "defers/"
sys.path.append(import_dir)
from common import *

data_dir = models_dir + "data/"

omap = {"LC": "I", "PR": "II"}

results_dir = {
  "LC": models_dir + "defers/pymc3/" + "originator_" + omap["LC"] + "/",
  "PR": models_dir + "defers/pymc3/" + "originator_" + omap["PR"] + "/"
}

idx = pd.IndexSlice

ASOF_DATE = datetime.date(2020, 6, 11)
dep_var = "distress"

fname = data_dir + "claims.pkl"
with open(fname, "rb") as f:
    claims_dict = joblib.load(f)
@


\title{Loan Payment Distress Due to COVID-19: A Case Study}
\author{Gyan Sinha, Godolphin Capital Management, LLC%
\thanks{\scriptsize \emph{%Godolphin Capital Management, LLC,%
\href{mailto:gsinha@godolphincapital.com}{Email Gyan}. This report
has been prepared by Godolphin Capital Management, LLC
(``Godolphin'') and is provided for informational purposes only and
does not constitute an offer to sell or a solicitation to purchase
any security. The contents of this research report are not intended
to provide investment advice and under no circumstances does this
research report represent a recommendation to buy or sell a security.
The information contained herein reflects the opinions of Godolphin.
Such opinions are based on information received by Godolphin from
independent sources. While Godolphin believes that the information
provided to it by its sources is accurate, Godolphin has not independently
verified such information and does not vouch for its accuracy. Neither
the author, nor Godolphin has undertaken any responsibility to update
any portion of this research report in response to events which may
transpire subsequent to its original publication date. As such, there
can be no guarantee that the information contained herein continues
to be accurate or timely or that Godolphin continues to hold the views
contained herein. Godolphin is an investment adviser. Investments
made by Godolphin are made in the context of various factors including
investment parameters and restrictions. As such, there may not be
a direct correlation between the views expressed in this article and
Godolphin's trading on behalf of its clients. 
<%print(f'Version:{datetime.datetime.now()}') %>}}
}

\date{\today}
%{\includegraphics[width=.25\textwidth]{GodolphinLogo.jpg}}

\begin{document}
\maketitle

\begin{abstract}

This report analyzes payment distress or forebearance as a result of
COVID-19 related shutdowns in the US. We focus on a
portfolio of unsecured consumer loans originated by 2 different
institutions. Our analysis focuses on a few key questions:
\begin{itemize}
\item what is the magnitude of COVID related distress?
\item are there systematic relationships between loan attributes and
  payment distress?
\item how are labor market trends related to the probability
  of loan distress? 
\item does the sensitivity to labor market shocks vary by region?
\end{itemize}

The results presented here are intended to provide a basis for discussion 
within a general framework that can be applied not only to unsecured consumer 
loans but also more broadly, to other lending sectors. While the data are still 
preliminary and the events they capture very recent, our conclusions are based on 
a rigorous and transparent statistical analysis and presented with confidence 
bounds that respect the uncertainty we are currently living through.

In summarizing our results, we would say that larger loans are at greater risk of 
distress while more seasoning (the age of the loan) tends to reduce the chances. 
Borrowers who rent and are self-employed are at higher risk. Loans 
with 5-year amortization terms are riskier than those with 3-year terms. All else 
held equal, the impact of FICO scores, income and DTI ratios is ambiguous ---  in one case, 
they tend to decrease risk while in the other they tend to raise it, albeit by 
small amounts. On average, a 1 standard-deviation increase in the annual percentage
change in weekly initial claims imply roughly 8\% to 30\% increases in distress 
probabilities, depending on originator and even within originators, there is substantial 
variation across states.

\end{abstract}

<<out_dicts, echo=False>>=

def read_results(model_type, originator, asof_date):
    ''' read pickled results '''

    import_dir = models_dir + "defers/"
    fname = (
        import_dir + "pymc3/originator_" + originator + "/results/" + 
        "_".join(["defer", originator, model_type, asof_date.isoformat()])
      )
    fname += ".pkl"

    with open(fname, "rb") as f:
        out_dict = joblib.load(f)

    return out_dict

out_dict = {}
pipe_dict = {}
scaler_dict = {}
obs_covars_dict = {}
hard_df_dict = {}
test_dict = {}

for i in [omap["LC"], omap["PR"]]:
    for j in ["pooled", "hier"]:
        out_dict[":".join([i, j])] = read_results(j, i, ASOF_DATE)
        
    orig_model_key = ":".join([i, "hier"])
    pipe_dict[i] = {
        "stage_one": out_dict[orig_model_key]["pipe"]["p_s_1"],
        "stage_two": out_dict[orig_model_key]["pipe"]["p_s_2"],
        "stage_three": out_dict[orig_model_key]["pipe"]["p_s_3"],
        "stage_four": out_dict[orig_model_key]["pipe"]["p_s_4"],
    }
    scaler_dict[i] = (
        pipe_dict[i]["stage_two"].named_steps.std_dummy.numeric_transformer.named_steps["scaler"]
    )
    obs_covars_dict[i] = out_dict[orig_model_key]["obs_covars"]
    hard_df_dict[i] = out_dict[orig_model_key]["hard_df"]
    test_dict[i] = out_dict[orig_model_key]["test"]

risk_df = gen_labor_risk_df(
    "articles_spreadsheet_extended.xlsx", data_dir
)


@

<<pymc_funcs, echo=False>>=

def make_az_data(originator, model_type):
    ''' make az data instance for originator '''

    orig_model_key = ":".join([originator, model_type])
    model = out_dict[orig_model_key]["model"]
    trace = out_dict[orig_model_key]["trace"]

    pipe_stage_two = pipe_dict[originator]["stage_two"]
    pipe_stage_three = pipe_dict[originator]["stage_three"]
    pipe_stage_four = pipe_dict[originator]["stage_four"]
  
    t_covars = pipe_stage_four.named_steps.spline.colnames

    if model_type == "pooled":
        b_names = ["γ"] + t_covars + pipe_stage_two.named_steps.std_dummy.col_names
        az_data = az.from_pymc3(trace=trace, model=model, coords={'covars': b_names}, dims={'b': ['covars']})
        st_out = pd.DataFrame()

        b_out = az.summary(az_data, round_to=3, var_names=["b"])
        b_out.index = b_names
    else:
        state_fips_indexes_df = pipe_stage_three.named_steps.hier_index.grp_0_grp_1_indexes_df
        index_0_to_st_code_df = state_fips_indexes_df.drop_duplicates(subset=["st_code"])[
        ["index_0", "st_code"]].set_index("index_0")

        index_0_to_st_code_df = pd.merge(index_0_to_st_code_df, states_df, on="st_code")
        b_names = pipe_stage_two.named_steps.std_dummy.col_names[:-1]
        c_names = ["γ"] + t_covars +  ["η"]
        az_data = az.from_pymc3(
            trace=trace, model=model, 
            coords={'obs_covars': b_names, "pop_covars": c_names, 'st_code': index_0_to_st_code_df.state.to_list()},
            dims={'b': ['obs_covars'], "g_c_μ": ["pop_covars"], "g_c_σ": ["pop_covars"], "st_c_μ": ["st_code"], "st_c_μ_σ": ["st_code"]}
        )
        st_out = az.summary(az_data, var_names=["st_c_μ"], round_to=3)
        st_out_idx = pd.MultiIndex.from_tuples(
            [(x, y) for x in index_0_to_st_code_df.state.to_list() for y in c_names],
            names=["state", "param"]
        )
        st_out.index = st_out_idx

        b_out = az.summary(az_data, round_to=3, var_names=["b"])
        b_out.index = b_names

    return trace, az_data, st_out, b_out, t_covars
@

<<datasets, echo=False>>=
# just need one issuer for the combined data
hard_df = hard_df_dict[omap["LC"]]

ic_date = (
  pipe_dict[omap["LC"]]["stage_one"].named_steps.add_state_macro_vars.ic_long_df["edate"].max().date()
)

numeric_features = [
    "fico", "original_balance", "dti", "stated_monthly_income", "age", "pct_ic"
]
categorical_features = [
    "grade", "purpose", "employment_status", "term", "home_ownership", "is_dq"
]

knots = np.linspace(0., 15., 7)

data_scaler_dict = {}
for i in [omap["LC"], omap["PR"]]:
    data_scaler_dict[i] = {
        "mu" : dict(zip(numeric_features, scaler_dict[i].mean_)),
        "sd":  dict(zip(numeric_features, scaler_dict[i].scale_))  
    }
@

\section{Introduction}

Our reasons for undertaking this research project were driven by
practical considerations --- like many other investors in consumer and
mortgage lending, we happen to be long these loans. As such, it is
critical for us to evaluate future losses and prospective returns on
these loans and make assessments about their ``fundamental'' value.
We do this with the explicit recognition of the unprecedented nature
of the COVID shock and the fact that in many ways, we are sailing
through uncharted waters.

A natural question that may arise is the applicability of the analysis
to a broader context. While there is a natural
tendency to always seek out more and greater amounts of data, in
practice, investors in most cases, hold narrow subsets of the overall population of
loans. While larger datasets may give us more precise estimates (up to
a point), the fact is that we want to make statements about OUR
portfolio, not a fictional universe which is not owned by anyone in
particular. The challenge then is to employ statistical methods that
allow us to extract information from ``small'' not ``big'' data and
turn these into useful insights for decision-making. This is where the
bayesian methods we deploy in this report come in useful since they
explicitly deal with inferential uncertainty in an intrinsic way and
can be used to provide insights in other contexts as well.

There are 2 parts to our project. First, we 
describe the data set in some detail and present
stratifications by different loan attributes. We also
present the distress rates within each strata in order to get
intuition around the impact of loan attributes. We then
provide statistics around the labor markets in various states. We look
at the impact of the annual percentage change in initial claims, 
starting March 14th (which we peg as the start of the COVID crisis for our 
purposes) and through the week ending <%print(f'{ic_date.strftime("%B %-d, %Y")}')%>. 
An open question that the modeling seeks to answer is the impact of the
claims variables on distress rates and whether these can be leveraged into a 
prediction framework going forward. A discussion of the statistical model 
that relates the observed outcome (is the loan distressed: Yes/No?) to the 
various loan attributes is provided next. The framework employed is based on 
Survival Analysis, using a hierarchical bayes approach as
in~\cite{8328358dab6746d884ee538c687aa0dd}
and~\cite{doi:10.1198/004017005000000661}. In closing this part, we
present and discuss the results across the two institutions,
highlighting any differences in the impact of attributes that emerge.

In the second part of our work, we develop a methodology for
forecasting the path of initial claims at the national and state
levels over the next few months. This analysis is unique in its own
way and leverages a brief descriptive note put out by Federal Reserve
Bank of NY researchers in a blog article. We use the claims forecast
as inputs into the predictions for distress rates at the end of
second quarter of 2020, which is our forecast horizon. The model 
and the estimation results and forecasts are provided in an 
accompanying piece.

Before we dive into the details, there are 3 key technical aspects in
this report that are worth highlighting.  First, the use of Survival or
Hazard models to estimate the marginal distress probability, as a
function of weeks elapsed since the crisis, is \textbf{key} to sensible
projections of distress \footnote{This is a benefit over and above
the intrinsic gain from using this framework in the context of 
``censored'' data where most of the observations have not yet 
experienced distress}. As we show, these marginal
hazards have a very strong ``duration'' component which impacts
longer-term forecasts of the cumulative amount of distress we expect
in the future. 

Second, we extend the survival model framework by incorporating parameter 
hierarchies (within a bayesian framework) that explicitly account for random 
variation in the impact of variables, across state clusters. This allows for the 
possibility of ``unobserved heterogeneity'' in the data by
explicitly modeling a state-specific random variable that interacts
with and modifies the hazards for loan clusters within a state. This 
is an important enhancement since (i) there may be
differences in the composition of the workforce across states that
impact the way in which a given volume of claims affect distress
rates, and (ii) the borrower base itself may differ across states in both
observable and unobservable ways. We control for the observed
attributes explicitly but the hierarchical framework allows us to
model unobserved factors as well. 

Third, we develop a statistical framework 
to model ``decay'' rates for weekly claims and the role that labor markets 
play in determining distress rates, building upon ideas first discussed 
by researchers at the NY Fed. The projections from this framework serve as 
inputs to our longer-term distress forecasts and allows us to model the 
impact of different economic scenarios in the future, an important tool to have
in the arsenal given the considerable uncertainties that still remain
regarding the future path of the economy.

\section{Data}
In Table~\ref{tbl:portfolio_summary}, we provided an overview of our 
data sample. In all, we have <%print(f'{hard_df.shape[0]}')%> loans
in our data, in roughly a 50/50 split (by count) across the 2 institutions.

\begin{table}[ht]
\centering
\caption{Portfolio Summary}
\label{tbl:portfolio_summary}
\scalebox{0.80}{
<<portfolio_summary, echo=False, results="tex">>=

#hard_df["current_balance"] = (
#  hard_df["original_balance"] * hard_df["cur_note_amount"]/hard_df["note_amount"]
#)
hard_df["defer_dollar"] = hard_df[dep_var] * hard_df["current_balance"]

# def wavg(x):
#    return np.average(
#        x, weights=hard_df.loc[x.index, "current_balance"]
#    )

def wavg(x):
    return np.nansum(
        x * hard_df.loc[x.index, "current_balance"], axis=0
    )/np.nansum(hard_df.loc[x.index, "current_balance"])

aaa = hard_df.groupby(["originator", "grade"]).agg(
    n=('loan_id', "count"),
    original_balance=('original_balance', sum),
    current_balance=('current_balance', sum),
    wac=('original_rate', wavg),
    age=('age', wavg),
    fico=('fico', wavg),
    term=('original_term', wavg),
    dti=('dti', wavg),
    income=('stated_monthly_income', wavg),
    distress=(dep_var, wavg),
)

bbb = hard_df.groupby(["originator"]).agg(
    n=('loan_id', "count"),
    original_balance=('original_balance', sum),
    current_balance=('current_balance', sum),
    wac=('original_rate', wavg),
    age=('age', wavg),
    fico=('fico', wavg),
    term=('original_term', wavg),
    dti=('dti', wavg),
    income=('stated_monthly_income', wavg),
    distress=(dep_var, wavg),
)

bbb.index = pd.MultiIndex.from_tuples(
    [(omap["LC"], 'ALL'), (omap["PR"], 'ALL')], names=['originator', 'grade']
)

aaa = pd.concat([aaa, bbb])

ccc = pd.concat(
    [
        pd.Series(hard_df["loan_id"].apply("count"), name="n"),
        pd.Series(hard_df["original_balance"].sum(), name="original_balance"),
        pd.Series(hard_df["current_balance"].sum(), name="current_balance"),
        hard_df[
          [
            "original_rate", "age", "fico", "original_term", "dti", 
            "stated_monthly_income"
          ]
        ].apply(wavg).to_frame().T.rename(
            columns={
              "original_term": "term", "original_rate": "wac", "dti": "dti",
              "stated_monthly_income": "income"
            }
        ),
        pd.Series(wavg(hard_df[dep_var]), name=dep_var)
    ], axis=1
)
ccc.index = [('ALL', 'ALL')]

ddd = pd.concat([aaa, ccc])
ddd["pct"] = ddd["current_balance"]/ddd.loc[pd.IndexSlice["ALL", "ALL"],  "current_balance"]
ddd.index.names = ["Originator", "Grade"]

cfmt = "".join(["r"] * (ddd.shape[1] + 2))
header = [
  "N", "Orig. Bal.", "Cur. Bal.", "WAC", "WALA", "FICO", 
  "WAOT", "DTI", "Income", "Distress", "Share",
]

tbl_fmt = {
  "original_balance": utils.dollar,
  "current_balance": utils.dollar,
  "n": utils.integer, "fico": utils.number,
  "term": utils.number, "age": utils.number,
  "pct": utils.percent, "distress": utils.percent,
  "wac": utils.percent, "dti": utils.number,
  "income": utils.dollar
}

print(
    ddd.to_latex(
      index=True, multirow=True, 
      header=header,
      formatters=tbl_fmt,
      column_format=cfmt,
      multicolumn_format="r",
    ))

one_line = ddd.loc[pd.IndexSlice["ALL", :], :]
@
}
\end{table}

The aggregate original amount issued is \$%
<%print(f'{float(one_line.original_balance):,.2f}')%>,
with a weighted-average interest rate of
<%print(f'{utils.number(100*float(one_line.wac))}')%>\%,
a weighted-average FICO score of %
<%print(f'{utils.number(float(one_line.fico))}')%> and
is <%print(f'{utils.number(float(one_line.age))}')%>
months seasoned. The weighted-average original-term %
is <%print(f'{utils.number(float(one_line.term))}')%> months.
\textbf{Overall, the distress rate on this portfolio is %
<%print(f'{100*float(one_line.distress):.2f}')%>\%.}

The portfolio statistics presented here are as of
<%print(f'{ASOF_DATE.strftime("%B %-d, %Y")}')%> which is more than
one month into the onset of the significant ''shelter-at-home'' orders
across the country and resulting economic disruptions. Since 
most of these payment deferrals are for anywhere from 1 to 
3 months, the distress percentages can be viewed as the
cumulative share of loans deferred or delinquent since the start of the
COVID crisis. By way of comparison, we provide recent deferment figures for other 
related sectors such as mortgages. Approximately 8.46\% of all mortgage loans
were in forebearance as of May 24th, 2020 which is a roughly
<%print(f'{(pd.to_datetime(ASOF_DATE) - pd.to_datetime("2020-05-24")).days}') %>
days earlier than the cutoff date for our data set. In the Ginnie Mae
sector, 11.82\% of loans were in forebearance while the comparable
figure for conventional mortgages was 6.39\%.

In figure~\ref{fig:due_day_dist}, we present the frequency 
distribution of the payment dates on the loans in our sample.
Since borrowers may have a tendency to hold off
on requesting a deferral until they are close to or past their
due day, and given the relatively short data window, this may
lead to biases. A relatively uniform distribution of payment
due dates would serve to assuage this concern. Thankfully,
this is exactly what we find in the data presented here,
eliminating this aspect of the data as a potential source
of concern.

\begin{figure}
\caption{Distribution of due dates}
\label{fig:due_day_dist}
\scalebox{1}{
<<due_day_dist, echo=False>>=
pos = []
for i in [omap["LC"], omap["PR"]]:
    pos.append(get_due_day(i, ASOF_DATE))
pos_df = pd.concat(pos, ignore_index=True)
pos_df = pos_df[pos_df["loan_id"].isin(hard_df["loan_id"].to_list())]

fig, ax = plt.subplots(2, 1, figsize=(10, 5), sharey=True)
for i, v in enumerate([omap["LC"], omap["PR"]]):
    df = pos_df[pos_df["originator"] == v]
    ax[i].hist(df.pmt_day)
    ax[i].set_xlabel("Due day")
    ax[i].set_ylabel("Frequency")
    ax[i].set_title(f"Originator: {v}")
    
plt.tight_layout()
@
}
\end{figure}

In Table~\ref{tbl:port_summary_purpose}, we provide a stratification of 
the portfolio by loan purpose. More than two-thirds of the loans are used for
consolidating existing debt, mostly drawn on credit cards. The second 
largest category is for purchases, while less than 10\% is used for 
expenses such as for education, wedding etc. (``LifeCyle'').

\begin{table}[ht]
\centering
\caption{Portfolio summary, by purpose}
\label{tbl:port_summary_purpose}
\scalebox{0.7}{
<<port_summary_purpose, echo=False, results="tex">>=
purpose_tbl = summary_by_group(
    ["originator", "purpose"], hard_df
)
purpose_tbl.index.names = ["Originator", "Purpose"]
cfmt = "".join(["r"] * (purpose_tbl.shape[1] + 2))

print(
    purpose_tbl.to_latex(
      index=True, multirow=True, 
      header=header,
      formatters=tbl_fmt,
      column_format=cfmt,
      multicolumn_format="r",
    ))
@
}
\end{table}

In Table~\ref{tbl:port_summary_emp_status}, a stratification across 
the borrower's employment status is provided. The ``Self-employed''
and ``Other'' categories generally comprise anywhere from 10\% to 15\%
of the portfolio\footnote{In the case of Originator I, the employment
category is really a dummy variable for the presence or absence
of employment history --- if there is information on this count,
this field is coded as ``Employed'' otherwise it is coded as 
``Other''}.

\begin{table}[ht]
\centering
\caption{Portfolio summary, by employment status}
\label{tbl:port_summary_emp_status}
\scalebox{0.75}{
<<port_summary_emp_status, echo=False, results="tex">>=
emp_tbl = summary_by_group(
    ["originator", "employment_status"], hard_df
)
emp_tbl = emp_tbl.fillna(0)
emp_tbl.index.names = ["Originator", "Employment"]
cfmt = "".join(["r"] * (emp_tbl.shape[1] + 2))
print(
    emp_tbl.to_latex(
      index=True, multirow=True, 
      header=header,
      
      formatters=tbl_fmt,
      column_format=cfmt,
      multicolumn_format="r",
    ))
@
}
\end{table}

In Table~\ref{tbl:port_summary_homeowner}, the portfolio is stratified 
across housing tenure. Across the 2 institutions, roughly a quarter to
two-thirds of the borrowing is by renters.

\begin{table}[ht]
\centering
\caption{Portfolio summary, by homeownership}
\label{tbl:port_summary_homeowner}
\scalebox{0.75}{
<<port_summary_homeowner, echo=False, results="tex">>=
homeowner_tbl = summary_by_group(
    ["originator", "home_ownership"], hard_df
)
homeowner_tbl.index.names = ["Originator", "Housing"]
cfmt = "".join(["r"] * (homeowner_tbl.shape[1] + 2))
print(
    homeowner_tbl.to_latex(
      index=True, multirow=True, 
      header=header,
      
      formatters=tbl_fmt,
      column_format=cfmt,
      multicolumn_format="r",
    ))
@
}
\end{table}

Finally, in Table~\ref{tbl:port_summary_term}, we stratify by
loan term. Across the 2 institutions, roughly 50\% - 70\% of
the loans are for 3-year amortization terms, with the remainder
for a 5-year term.

\begin{table}[ht]
\centering
\caption{Portfolio summary, by term}
\label{tbl:port_summary_term}
\scalebox{0.75}{
<<port_summary_term, echo=False, results="tex">>=
term_tbl = summary_by_group(
    ["originator", "original_term"], hard_df
)
term_tbl.index.names = ["Originator", "Term"]
cfmt = "".join(["r"] * (term_tbl.shape[1] + 2))
print(
    term_tbl.to_latex(
      index=True, multirow=True, 
      header=header,
      
      formatters=tbl_fmt,
      column_format=cfmt,
      multicolumn_format="r",
    ))
@
}
\end{table}

An important question, with possible implications about the
prospective cure rates for the group of distressed loans, is 
what they look like versus the subset of loans that were already
delinquent before the crisis. This is presented in 
Table~\ref{tbl:pre_covid_dq_profile}.

\begin{table}[ht]
\centering
\caption{DQ \emph{vs} COVID profile}
\label{tbl:pre_covid_dq_profile}
\scalebox{0.75}{
<<pre_covid_dq_profile, echo=False, results="tex">>=
dq_tbl = summary_by_group(
    ["originator", "dq_grp"], hard_df
)
dq_tbl.index.names = ["Originator", "DQ Status"]
cfmt = "".join(["r"] * (dq_tbl.shape[1] + 2))
print(
    dq_tbl.to_latex(
      index=True, multirow=True, 
      header=header,
      
      formatters=tbl_fmt,
      column_format=cfmt,
      multicolumn_format="r",
    ))
@
}
\end{table}

The deferment subset (labeled ``Covid'') has better credit quality (as measured)
by their FICO scores than both ``Current'' and delinquent sub-population 
for Originator I. This may imply that the cure rate from deferments may be
better on the deferred sub-population than has been the experience on the 
delinquent sub-population. In the case of Originator II, the deferment and
delinquent sets have roughly the same FICO score which is lower
than that on the set of loans that are ``Current''.

\section{Employment}
The economic disruption caused by COVID is in many ways unusual 
in that it strikes at the Consumption component of overall GDP. 
As such, the disruption is much broader than would be the case,
say, for an investment led recession, caused by a contraction in
an isolated segment of the economy. 

In some ways, this resembles the 2008 recession which was caused by a 
massive asset writedowns in the banking sector (on a global basis) 
leading to an economy-wide credit crunch. To the extent that it strikes 
at almost two-thirds of overall economic output, the disruption is naturally 
even larger, as has become obvious in the labor market figures released over 
the last month. Labor markets are likely to be the key to explaining distress, and 
both the full magnitude of job losses and how quickly they are reversed
is going to be the driver of ultimate loan performance.

The trend in annualized percentage change in weekly initial claims 
and their distribution is presented in Figure~\ref{fig:claims_pct_trend}.
When we first did this figure, we thought we had made a mistake but the
percentage changes depicted here are correct - on an year-over-year basis,
initial claims did really increase by aproximately 8000\% at their peak
in early April!

\begin{figure}[ht]
\caption{Weekly Claims (Year-over-Year pct. change): trend and distribution}
\label{fig:claims_pct_trend}
\scalebox{1}{
<<claims_pct_trend, echo=False>>=

stage_one_df = pipe_dict[omap["LC"]]["stage_one"].fit_transform(hard_df)

fig, ax = plt.subplots(2, 1, figsize=(8, 6.4))

sns.boxplot(stage_one_df.sdate.dt.date, stage_one_df.pct_ic, ax=ax[0])
sns.distplot(stage_one_df.pct_ic, ax=ax[1], kde=False)

ax[0].set_xlabel("Week ending: "); 
ax[0].set_ylabel("Year-over-Year pct. change")
ax[1].set_xlabel("Year-over-Year pct. change")
ax[0].yaxis.set_major_formatter(mtick.PercentFormatter(1.0))
ax[1].xaxis.set_major_formatter(mtick.PercentFormatter(1.0))

ax[0].set_xticklabels(ax[0].get_xticklabels(), rotation=45, ha='right')

plt.tight_layout()
@
} 
\end{figure}

\subsection{Claims}
In Figures~\ref{fig:lc_defer_hazard_by_state} and
~\ref{fig:pr_defer_hazard_by_state}, we depict the 
relationship between payment distress and the claims measure.
The solid trend line is a robust fit for the scatter plot 
depicted here. The patterns are quite different across the
2 originators --- in one case, they appear to show no (or even a slightly negative)
relationship, while in the other, there appears to be a small positive
effect. In both cases, there is considerable variation across states.

<<defer_hazard_by_state, echo=False>>=

def st_map(originator):
    pipe_stage_three = pipe_dict[originator]["stage_three"]
    state_fips_indexes_df = pipe_stage_three.named_steps.hier_index.grp_0_grp_1_indexes_df
    index_0_to_st_code_df = state_fips_indexes_df.drop_duplicates(
        subset=["st_code"]
    )[["index_0", "st_code"]].set_index("index_0")
    index_0_to_st_code_df = pd.merge(index_0_to_st_code_df, states_df, on="st_code")
    
    st_out = pd.DataFrame(
        out_dict[originator + ":hier"]["trace"]["st_c_μ"][:, :, [0, -1]].mean(axis=0),
        columns=["α", "β"]
    )
    st_out["state"] = index_0_to_st_code_df.state.to_list()
    st_out["originator"] = originator

    g_out = out_dict[originator + ":hier"]["trace"]["g_c_μ"][:, [0, -1]].mean(axis=0)

    return st_out, g_out


def plot_defer_hazard_by_state(originator):
  ''' plots deferment hazards by state and week '''
  zzz = pipe_dict[originator]["stage_one"].fit_transform(hard_df)
  zzz.reset_index(inplace=True)

  a_df = zzz.groupby(["state"]).agg(
    n=("loan_id", "count"), k=(dep_var, np.sum), distress=(dep_var, np.mean),
    pct_ic=("pct_ic", np.mean)
  ).reset_index()

  g = sns.FacetGrid(
    data=a_df.reset_index(),
  )
  g.map(sns.regplot, "pct_ic", dep_var, ci=True)
  g.ax.xaxis.set_major_formatter(mtick.PercentFormatter(1.0))
  # g.ax.yaxis.set_major_formatter(mtick.PercentFormatter(1.0))

  # add annotations one by one with a loop
  for line in range(0, a_df.shape[0]):
    g.ax.text(
      a_df["pct_ic"][line]+0.001, a_df[dep_var][line], a_df["state"][line], 
      horizontalalignment='left', size='medium', color='red', 
      weight='semibold', alpha=0.25
    )

  g.ax.figure.set_size_inches(10, 5)
  g.ax.set_xlabel("Year-over-Year pct. change")
  g.ax.set_ylabel("Distress hazard")

  return g
@

\begin{figure}[htb!]
\caption{Originator I: deferment hazard}
\label{fig:lc_defer_hazard_by_state}
\scalebox{1}{
<<lc_defer_hazard_by_state, echo=False>>=
g = plot_defer_hazard_by_state(omap["LC"])
sns.despine(left=True)
@
}
\end{figure}

\begin{figure}[htb!]
\caption{Originator II: deferment hazard}
\label{fig:pr_defer_hazard_by_state}
\scalebox{1}{
<<pr_defer_hazard_by_state, echo=False>>=
g = plot_defer_hazard_by_state(omap["PR"])
sns.despine(left=True)
@
}
\end{figure}

The modeling exercise will seek to examine how much of the difference in 
slopes and the variability across states can be explained by individual
loan attributes and unobservable effects modeled as ``random effects''.

<<plot_fn, echo=False>>=
def create_state_aggs(originator, hard_df, risk_df):
    ''' pct deferment vs pct low risk '''
    
    xbar_df = hard_df.groupby(["originator", "state"]).agg(
      n=("loan_id", "count"), k=('defer', np.sum),
      pct=('defer', np.mean), balance=('cur_note_amount', sum)
    ).loc[pd.IndexSlice[originator, :], :].droplevel(0).reset_index()
    
    xbar_df = pd.merge(xbar_df, risk_df, on="state")
    
    return xbar_df
@

\section{Model}
The modeling framework used in this report draws upon statistical
tools used in the analysis of events with a ``time-until'' component
to them. In our case, the time-until, or ``lifetime''  we are 
interested in predicting is the time until a borrower asks the
servicer for a deferment or goes delinquent. Time in this context is measured from
an assumed epoch start date of March 14th, 2020 which we assume
as the start of the COVID-19 crisis for our purposes and is the
same across all loans. 

We incorporate state-based differences in the distribution
of hazard rates that manifest themselves in both the pattern of duration dependence 
as well as the impact that changes in state-level initial claims rates have on hazards. 
Duration dependence is captured using Basis Splines, with 5 knots set at the quantiles
of the event-time distribution for loans which experienced an event. All numerical
covariates are standardized by subtracting the mean and dividing by the standard 
deviation. Categorical features are encoded using ``dummy variable'' where the
first category is treated as the baseline.

The  model is calibrated to a ``training'' data set that consists of a
random sample of 80\% from the full dataset, stratified on state. The posterior samples
derived from the bayesian estimator are then used to derive predictions for the
remaining 20\% ``test'' sample. 

Further details are provided in the appendix.

\section{Estimates}
We now turn to a discussion of the results. We use the \href{https://docs.pymc.io/}{PyMC3} Python
package to estimate the model. The parameter estimates are presented for each 
originator in turn.

<<compare_table, echo=False, include=False>>=
def get_pop(originator, az_data, param):
    ''' compare originators'''
    
    pipe_stage_four = pipe_dict[originator]["stage_four"]
    t_covars = pipe_stage_four.named_steps.spline.colnames
    c_names =  ["γ"] + t_covars + ["η"]
    
    pipe_stage_two = pipe_dict[originator]["stage_two"]
    b_names = pipe_stage_two.named_steps.std_dummy.col_names[:-1] 
    
    dat = az_data[originator]
    
    if param == "b":
        g_out = az.summary(dat, var_names=["b"], round_to=3)
        g_out.index = b_names
        sub_names = ["fico", "original_balance", "dti", "stated_monthly_income", "age"]

        return pd.DataFrame(
            np.hstack(
                (
                    g_out.loc[sub_names, ["mean"]].values, 
                    g_out.loc[sub_names, ["sd"]].values
                )
            ), columns=["μ", "σ"], index=sub_names
        )
    else:
        g_out = az.summary(dat, var_names=["g_c_μ", "g_c_σ"], round_to=3)
        g_out.index = [x + "(μ)" for x in c_names] + [x + "(σ)" for x in c_names]
    
        return pd.DataFrame(
            np.hstack(
                (
                    g_out.iloc[:7, [0]].values, 
                    g_out.iloc[7:, [0]].values
                )
            ), columns=["μ", "σ"], index=c_names
        )
az_data = {}
for i in [omap["LC"], omap["PR"]]:
    az_data[i] = make_az_data(i, "hier")
@


\subsection{Hazards}
In this section, we present depictions of the marginal distress
probabilities (using 
\href{https://en.wikipedia.org/wiki/Nelson%E2%80%93Aalen_estimator}{Nelson-Aalen hazards}
), 
to set the stage for what we should expect our fully-specified hazards to look like.
The hazard estimates depicted in Figure~\ref{fig:nelson_aalen} were computed using the
\href{https://lifelines.readthedocs.io/en/latest/#}{Lifelines}
Python package.

\begin{figure}[ht]
\centering
\caption{Hazards}
\label{fig:nelson_aalen}
\scalebox{1}{
<<nelson_aalen, echo=False>>=

T = hard_df.dur
E = hard_df[dep_var]

bandwidth = 1
naf = NelsonAalenFitter()
lc = hard_df["originator"].isin([omap["LC"]])

naf.fit(T[lc],event_observed=E[lc], label="Originator I")
ax = naf.plot_hazard(bandwidth=bandwidth, figsize=(10, 5))

naf.fit(T[~lc], event_observed=E[~lc], label="Originator II")
naf.plot_hazard(ax=ax, bandwidth=bandwidth)

ax.set_xlabel("Weeks since March 14th, 2020")
ax.set_ylabel("Weekly hazard")

_  = plt.xlim(0, hard_df.dur.max() + 1)
@
}
\end{figure}

The hazards rose sharply in the first weeks after the start of the crisis,
to between 2\% and 2.5\%, but have declined since then. They reveal a difference
in operating protocols where it appears that in the case of Originator I, 
the initial flurry of claims were processed in a batch and then approved 
all at once in the second week. The overall pattern of events and censoring 
is presented in Table~\ref{tbl:survival_table_all}. The ``observed'' column
indicates the count of loans where distress was observed during
the interval specified in the ``event-at'' column on the left. Since the
study inception date is the same for all loans, a large fraction
of the data show up as ``censored''
in the last interval, and reported under the censored column. The other entries
in this column pertain to loans that either prepaid or were charged-off during
the period starting March 14th, 2020 and the cutoff date. The removed
column represents the portion of the ``at-risk'' population that is no longer
at risk since the loan was either censored or experienced an event. The
at-risk figure for the previous interval is decremented by the removed column
for that interval to give a new at-risk number.

Given these results, we model the duration dependence as a function of 
event-time as a Basis Spline in $t$. Details are provided in the appendix.

\begin{table}[ht]
\centering
\caption{Survival table: all loans}
\label{tbl:survival_table_all}
\scalebox{1}{
<<survival_table_all, echo=False, results="tex">>=
lt_df = lifelines.utils.survival_table_from_events(
    hard_df.dur, 
    hard_df[dep_var], collapse=True
)
print(lt_df.to_latex(column_format='rrrrr'))
@
}

\end{table}

\subsection{Originator I}

<<func_defs, echo=False>>=

def make_ppc_plot(originator):
    ''' make ppc plot '''
    
    orig_model_key = ":".join([originator, "hier"])
    ppc, s_1_df, _ = simulate(
        test_dict[originator], dep_var, claims_dict["chg_df"], originator,
        ASOF_DATE, out_dict[orig_model_key]
    )
    
    fig, ax = plt.subplots(figsize=(10, 5))

    ax.hist(ppc.mean(axis=0), bins=19, alpha=0.5)
    ax.axvline(s_1_df[dep_var].mean())

    ax.set(xlabel='Deferment hazard', ylabel='Frequency')
  
    pctile = np.percentile(ppc.mean(axis=0), q=[5, 95])
    ax.axvline(pctile[0], color="red", linestyle=":")
    ax.axvline(pctile[1], color="red", linestyle=":")

    _ = ax.text(
      1.65 * s_1_df[dep_var].mean(), 0.85 * ax.get_ylim()[1], 
      f'95% HPD: [{pctile[0]:.3f}, {pctile[1]:.3f}]'
    )

    return fig
@

\begin{comment}
We first provide a summary of the estimates for the pooled model, in
Table~\ref{tbl:lc_pooled_estimates} more for reference purposes than
anything else. The pooled model treats all observations as
being derived from the same underlying distribution ignoring the impact of differences
driven by region. 

\begin{table}
\caption{Originator I: pooled estimates}
\label{tbl:lc_pooled_estimates}
\scalebox{1}{
<<lc_pooled_estimates, echo=False, results="tex">>=

pooled_trace, pooled_data, _, pooled_b_out, t_covars = make_az_data(omap["LC"], "pooled")
print(
  pooled_b_out[["mean", "sd", "hpd_3%", "hpd_97%", "r_hat"]].to_latex(
     column_format="rrrrrr"
  )
)
@
}
\end{table}

\end{comment}

We examine the posterior predictive distribution
of the probability of the binary distress outcome variable versus the mean of the
observed outcome in the data. This is presented in Figure~\ref{fig:lc_ppc} where the vertical line
represents the observed distress percent while the barchart shows the distribution of
posterior predicted probabilities in the sample, together with the 95\% Highest Posterior
Density (HPD) interval. Note that these are hazards and not unconditional probabilities.

\begin{figure}[htb!]
\caption{Originator I distress rate: posterior predictive distribution}
\label{fig:lc_ppc}
\scalebox{1}{
<<lc_ppc, echo=False>>=
fig = make_ppc_plot(omap["LC"])
fig.show()
@
}
\end{figure}

The mean of the distribution of predicted hazards matches the average
distress rate in the sample quite well. We have also examined other 
standard metrics for measuring convergence for the MCMC sampler that support the
the validity of the sampling results presented here but have witheld
them in the interest of brevity.

\subsubsection{Population Means}
The population means, from which the random effects for each state are 
assumed to be drawn, are presented in Table~\ref{tbl:lc_pop_means}.
The intercept term ($\gamma$) and the basis spline coefficients
$t_{0}$ through $t_{4}$ encapsulate the baseline hazard for the
``average'' loan in the population. The $\eta$ term represents the
average impact of a 1 standard-deviation move in the year-over-year
percentage change in weekly initial claims. 

\begin{table}[ht]
\centering
\caption{Originator I: population means for random effects}
\label{tbl:lc_pop_means}
\scalebox{1}{
<<lc_global_mean, echo=False, results="tex">>=

hier_trace, hier_data, lc_hier_st_out, lc_hier_b_out, t_covars = make_az_data(
  omap["LC"], "hier"
)

c_names =  ["γ"] + t_covars + ["η"]
g_out = az.summary(hier_data, var_names=["g_c_μ"], round_to=3)
g_out.index = [x + "(μ)" for x in c_names]
print(g_out[["mean", "sd", "hpd_3%", "hpd_97%", "r_hat"]].to_latex(
  index=True, column_format="rrrrrr"))
@
}
\end{table}

\subsubsection{State effects}

We examine next the impact of regional variation in state distress
rates --- this the first level in our 2-level model. 
The estimates of the impact of claims on  distress hazards are presented
in Figure~\ref{fig:lc_claims_sensitivity}, in the form of a choropleth
map of the US states. The sensitivities are grouped into quantiles and
color coded. 

\begin{figure}[ht]
\centering
\caption{Originator I: claims sensitivity}
\label{fig:lc_claims_sensitivity}
\scalebox{1}{
<<lc_claims_sensitivity, echo=False>>=

dff_η = lc_hier_st_out.loc[idx[:, "η"], "mean"].droplevel(level=1).reset_index().rename(
    columns={"mean": "value"}
)

us_states = gpd.read_file(
  "https://www2.census.gov/geo/tiger/GENZ2018/shp/cb_2018_us_state_20m.zip"
)
merged_us_states_η = pd.merge(
  us_states, dff_η, left_on="STUSPS", right_on="state", how="right")

fig, ax = plt.subplots(1, figsize=(15, 18))
albers_epsg = 2163
ax = us_states[~us_states["STATEFP"].isin(['02', '15'])].to_crs(epsg=albers_epsg).plot(
    ax=ax, linewidth=0.25, edgecolor='white', color='grey'
)

ax = merged_us_states_η[~merged_us_states_η["STATEFP"].isin(['02', '15'])].to_crs(epsg=albers_epsg).plot(
    column='value', ax=ax, cmap='viridis', scheme="quantiles",  legend=True, 
    legend_kwds={"loc": "upper center", "ncol": 3}
)
_ = ax.axis('off')
@
}

\end{figure}

In Figure~\ref{fig:lc_frailty}, we present the random ``intercept'' term for the
hazard model for each state. This is a measure of the unobservable charactersitics
that shift the individual hazard rates up or down, based on the membership within
a state grouping. Again the intercept terms are grouped into quantiles and 
presented color-coded on a choropleth map.

\begin{figure}[ht]
\centering
\caption{Originator I: frailty estimate}
\label{fig:lc_frailty}
\scalebox{1}{
<<lc_frailty, echo=False>>=

dff_γ = lc_hier_st_out.loc[idx[:, "γ"], :].droplevel(level=1).reset_index().rename(
    columns={"mean": "value"}
)

merged_us_states_γ = pd.merge(
  us_states, dff_γ, left_on="STUSPS", right_on="state", how="right"
)

fig, ax = plt.subplots(1, figsize=(15, 18))
albers_epsg = 2163
ax = us_states[~us_states["STATEFP"].isin(['02', '15'])].to_crs(epsg=albers_epsg).plot(
    ax=ax, linewidth=0.25, edgecolor='white', color='grey'
)

ax = merged_us_states_γ[~merged_us_states_γ["STATEFP"].isin(['02', '15'])].to_crs(epsg=albers_epsg).plot(
    column='value', ax=ax, cmap='viridis', scheme="quantiles", legend=True, 
    legend_kwds={"loc": "upper center", "ncol": 3}
)
_ = ax.axis('off');
@
}
\end{figure}



\subsubsection{Covariate effects}
In Figure~\ref{fig:lc_covar_effects_plot} and Table~\ref{tbl:lc_covariate_table}, 
we present the estimated coefficents for the covariates in our model. Note 
that these estimates act as ``shifters'' in that they move
the probability of distress for the baseline loan up or down. Since all our covariates are 
standardized (have a mean of 0 and a standard deviation of 1), the impact of 
a 1 standard-deviation move in the covariate on the ``odds-ratio'' ($\frac{p}{1-p}$) can be 
estimated by exponentiating the coefficent value.

\begin{figure}[hbt]
\centering
\caption{Originator I: covariate effects}
\label{fig:lc_covar_effects_plot}
\scalebox{1}{
<<lc_covar_effects_plot, echo=False>>=
ax = az.plot_forest(hier_data, var_names=["b"], combined=True, figsize=(10, 5))
grade_vars = [x for x in lc_hier_b_out.index if "grade" in x and not "fico" in x]
_ = ax[0].set_yticklabels(reversed(lc_hier_b_out.index.to_list()))
@
}
\end{figure}

We discuss the results for each covariate in turn:
\begin{description}
\item [Grade:] The credit grade estimates represent the impact of each grade relative to
  the reference category which defines the baseline loan. The odds-ratio 
  indicate relatively modest ``shifts'' for the 2nd category, but are 
  more onerous for the other lower-grade categories. The mean odds ratio of 
  <%print(f'{np.exp(lc_hier_b_out.loc[grade_vars[1], "mean"]):.2f}')%>,
  <%print(f'{np.exp(lc_hier_b_out.loc[grade_vars[2], "mean"]):.2f}')%> and
  <%print(f'{np.exp(lc_hier_b_out.loc[grade_vars[3], "mean"]):.2f}')%>
  indicate higher risks relative to the baseline loan. Note that
  we are controlliing for FICO scores as another covariate in the model so these
  effects are net of FICO composition effects within grades.

  \item [Purpose:] Loans taken out for asset puchases are
    <%print(f'{100*abs(np.exp(lc_hier_b_out.loc["C(purpose)[T.Acquisition]", "mean"])-1.0):.0f}')%>\% 
    riskier than the baseline ``Debt-Consolidation'' category. The ``LifeCycle''
    and the catch-all ``Other'' category are
    <%print(f'{100*abs(np.exp(lc_hier_b_out.loc["C(purpose)[T.LifeCycle]", "mean"])-1.0):.0f}')%>\%
    and
    <%print(f'{100*abs(np.exp(lc_hier_b_out.loc["C(purpose)[T.Other]", "mean"])-1.0):.0f}')%>\%
    less risky than the baseline category, respectively.
  
  \item [Employment status:] Borrowers categorized as ``Other'' for this variable are lower risk than 
    borrowers with recorded employment histories\footnote{In the case of Originator I, the 
    ``Self-employed'' tag is an empty dummy indicator with a value of 0 for all observations.}. 

  \item [Term:] 5-year loans are slightly higher risk than 3-year loans.

  \item [Homeownership:] Renters are about %
    <%print(f'{100*abs(np.exp(lc_hier_b_out.loc["C(home_ownership)[T.Rent]", "mean"])-1.0):.0f}')%>\%
    higher risk than borrowers that own their home.

  \item [FICO:] a 1 standard-deviation (about <%print(f'{data_scaler_dict[omap["LC"]]["sd"]["fico"]:.0f}')%> 
    points) increase in FICO, all else equal, causes the odds-ratio to increase by %
    <%print(f'{100*abs(np.exp(lc_hier_b_out.loc["fico", "mean"])-1.0):.0f}')%>\%.

  \item [Original Balance:] a 1 standard-deviation (about \$<%print(f'{data_scaler_dict[omap["LC"]]["sd"]["original_balance"]:,.0f}')%>)
    increase in original balance causes the odds-ratio to increase by %
    <%print(f'{100*abs(np.exp(lc_hier_b_out.loc["original_balance", "mean"])-1.0):.0f}')%>\%.

  \item [DTI:] a 1 standard-deviation (about <%print(f'{data_scaler_dict[omap["LC"]]["sd"]["dti"]:.2f}')%>\%)
    higher DTI causes the odds-ratio to decrease by %
    <%print(f'{100*abs(np.exp(lc_hier_b_out.loc["dti", "mean"])-1.0):.0f}')%>\%.

  \item [Income:]  a 1 standard-deviation (about \$<%print(f'{data_scaler_dict[omap["LC"]]["sd"]["stated_monthly_income"]:,.2f}')%>)
    increase in monthly income causes the odds-ratio to decrease by %
    <%print(f'{100*abs(np.exp(lc_hier_b_out.loc["stated_monthly_income", "mean"])-1.0):.0f}')%>\%.
  
  \item [Seasoning:] a 1 standard-deviation (about <%print(f'{data_scaler_dict[omap["LC"]]["sd"]["age"]:.0f}')%>
    months) increase in loan age causes the odds-ratio to decrease by %
    <%print(f'{100*abs(np.exp(lc_hier_b_out.loc["age", "mean"])-1.0):.0f}')%>\%.
  
\end{description}

\subsection{Originator II}

\begin{comment}

\begin{table}
\caption{Originator II: pooled estimates}
\label{tbl:pr_pooled_estimates}
\scalebox{1}{
<<pr_pooled_estimates, echo=False, results="tex">>=

pooled_trace, pooled_data, _, pooled_b_out, t_covars = make_az_data(omap["PR"], "pooled")
print(
  pooled_b_out[["mean", "sd", "hpd_3%", "hpd_97%", "r_hat"]].to_latex(
     column_format="rrrrrr"
  )
)
@
}
\end{table}
\end{comment}

We first examine the posterior predictive distribution
of the probability of the binary outcome variable versus
the mean of the observed outcome in the data. This is presented in
Figure~\ref{fig:pr_ppc}. 

\begin{figure}[htb!]
\caption{Originator II distress rate: posterior predictive distribution}
\label{fig:pr_ppc}
\scalebox{1}{
<<pr_ppc, echo=False>>=
fig = make_ppc_plot(omap["PR"])
fig.show()
@
}
\end{figure}

\subsubsection{Population Means}
The population mean estimates for Originator II are presented in
Table~\ref{tbl:pr_pop_means}. The interpretation of the coefficients
is similar to what we described earlier for Originator I.

\begin{table}[ht]
\centering
\caption{Originator II: population means for random effects}
\label{tbl:pr_pop_means}
\scalebox{1}{
<<pr_global_mean, echo=False, results="tex">>=

hier_trace, hier_data, pr_hier_st_out, pr_hier_b_out, t_covars = make_az_data(
  omap["PR"], "hier"
)

c_names =  ["γ"] + t_covars + ["η"]
g_out = az.summary(hier_data, var_names=["g_c_μ"], round_to=3)
g_out.index = [x + "(μ)" for x in c_names]
print(g_out[["mean", "sd", "hpd_3%", "hpd_97%", "r_hat"]].to_latex(
  index=True, column_format="rrrrrr"))
@
}
\end{table}

\subsubsection{State effects}
The estimates of the impact of claims on 
distress hazards for originator II are presented in 
Figure~\ref{fig:pr_claims_sensitivity}. 

\begin{figure}[ht]
\centering
\caption{Originator II: claims sensitivity}
\label{fig:pr_claims_sensitivity}
\scalebox{1}{
<<pr_claims_sensitivity, echo=False>>=

dff_η = pr_hier_st_out.loc[idx[:, "η"], "mean"].droplevel(level=1).reset_index().rename(
    columns={"mean": "value"}
)

us_states = gpd.read_file(
  "https://www2.census.gov/geo/tiger/GENZ2018/shp/cb_2018_us_state_20m.zip"
)
merged_us_states_η = pd.merge(
  us_states, dff_η, left_on="STUSPS", right_on="state", how="right")

fig, ax = plt.subplots(1, figsize=(15, 18))
albers_epsg = 2163
ax = us_states[~us_states["STATEFP"].isin(['02', '15'])].to_crs(epsg=albers_epsg).plot(
    ax=ax, linewidth=0.25, edgecolor='white', color='grey'
)

ax = merged_us_states_η[~merged_us_states_η["STATEFP"].isin(['02', '15'])].to_crs(epsg=albers_epsg).plot(
    column='value', ax=ax, cmap='viridis', scheme="quantiles",  legend=True, 
    legend_kwds={"loc": "upper center", "ncol": 3}
)
_ = ax.axis('off')
@
}
\end{figure}

\begin{figure}[ht]
\centering
\caption{Originator II: frailty estimate}
\label{fig:pr_frailty}
\scalebox{1}{
<<pr_frailty, echo=False>>=

dff_γ = pr_hier_st_out.loc[idx[:, "γ"], :].droplevel(level=1).reset_index().rename(
    columns={"mean": "value"}
)

merged_us_states_γ = pd.merge(
  us_states, dff_γ, left_on="STUSPS", right_on="state", how="right"
)

fig, ax = plt.subplots(1, figsize=(15, 18))
albers_epsg = 2163
ax = us_states[~us_states["STATEFP"].isin(['02', '15'])].to_crs(epsg=albers_epsg).plot(
    ax=ax, linewidth=0.25, edgecolor='white', color='grey'
)

ax = merged_us_states_γ[~merged_us_states_γ["STATEFP"].isin(['02', '15'])].to_crs(epsg=albers_epsg).plot(
    column='value', ax=ax, cmap='viridis', scheme="quantiles", legend=True, 
    legend_kwds={"loc": "upper center", "ncol": 3}
)
_ = ax.axis('off')
@
}

\end{figure}


\begin{figure}[hbt!]
\centering
\caption{Originator II: Predicted baseline distress probability}
\label{fig:pr_pred_base_prob}
\scalebox{1}{
<<pr_pred_base_prob, echo=False>>=

@
}
\end{figure}

\subsubsection{Covariate effects}

\begin{figure}[hbt!]
\centering
\caption{Originator II: covariate effects}
\label{tbl:pr_covar_effects_plot}
\scalebox{1}{
<<pr_covar_effects_plot, echo=False>>=
ax = az.plot_forest(hier_data, var_names=["b"], combined=True, figsize=(10, 5))
grade_vars = [x for x in pr_hier_b_out.index if "grade" in x and not "fico" in x]
_ = ax[0].set_yticklabels(reversed(pr_hier_b_out.index.to_list()))
@
}
\end{figure}

We discuss the results for each covariate in turn:
\begin{description}
\item [Grade:] The credit grade estimates represent the impact of each grade relative to
  the reference category which defines the baseline loan. The odds-ratio 
  indicate negative ``shifts'' for the 2nd and 3rd categories, but are 
  more onerous for the other lower-grade categories. 
  The mean odds ratio of 
  <%print(f'{np.exp(pr_hier_b_out.loc[grade_vars[1], "mean"]):.2f}')%>,
  <%print(f'{np.exp(pr_hier_b_out.loc[grade_vars[2], "mean"]):.2f}')%> and
  <%print(f'{np.exp(pr_hier_b_out.loc[grade_vars[3], "mean"]):.2f}')%>
  indicate relative risks of around 1.5 times the baseline loan. Note that
  we are controlliing for FICO scores as another covariate in the model so these
  effects are net of FICO composition effects within grades. 

  \item [Purpose:] Loans taken out for asset puchases are
    <%print(f'{100*abs(np.exp(pr_hier_b_out.loc["C(purpose)[T.Acquisition]", "mean"])-1.0):.0f}')%>\% 
    riskier than the baseline ``Debt-Consolidation'' category. The ``LifeCycle''
    and the catch-all ``Other'' category are
    <%print(f'{100*abs(np.exp(pr_hier_b_out.loc["C(purpose)[T.LifeCycle]", "mean"])-1.0):.0f}')%>\%
    and
    <%print(f'{100*abs(np.exp(pr_hier_b_out.loc["C(purpose)[T.Other]", "mean"])-1.0):.0f}')%>\%
    less risky than the baseline category, respectively.
  
  \item [Employment status:] Borrowers categorized as ``Other'' for this variable are lower risk than 
    borrowers with recorded employment histories\footnote{In the case of Originator I, the 
    ``Self-employed'' tag is an empty dummy indicator with a value of 0 for all observations.}. 

  \item [Term:] 5-year loans are slightly higher risk than 3-year loans.

  \item [Homeownership:] Renters are about %
    <%print(f'{100*abs(np.exp(pr_hier_b_out.loc["C(home_ownership)[T.Rent]", "mean"])-1.0):.0f}')%>\%
    higher risk than borrowers that own their home.

  \item [FICO:] a 1 standard-deviation (about <%print(f'{data_scaler_dict[omap["PR"]]["sd"]["fico"]:.0f}')%> 
    points) increase in FICO, all else equal, causes the odds-ratio to decrease by %
    <%print(f'{100*abs(np.exp(pr_hier_b_out.loc["fico", "mean"])-1.0):.0f}')%>\%.

  \item [Original Balance:] a 1 standard-deviation (about \$<%print(f'{data_scaler_dict[omap["PR"]]["sd"]["original_balance"]:,.0f}')%>)
    increase in original balance causes the odds-ratio to increase by %
    <%print(f'{100*abs(np.exp(pr_hier_b_out.loc["original_balance", "mean"])-1.0):.0f}')%>\%.

  \item [DTI:] a 1 standard-deviation (about <%print(f'{data_scaler_dict[omap["PR"]]["sd"]["dti"]:.2f}')%>\%)
    higher DTI causes the odds-ratio to increase by %
    <%print(f'{100*abs(np.exp(pr_hier_b_out.loc["dti", "mean"])-1.0):.0f}')%>\%.

  \item [Income:]  a 1 standard-deviation (about \$<%print(f'{data_scaler_dict[omap["PR"]]["sd"]["stated_monthly_income"]:,.2f}')%>)
    increase in monthly income causes the odds-ratio to increase by %
    <%print(f'{100*abs(np.exp(pr_hier_b_out.loc["stated_monthly_income", "mean"])-1.0):.0f}')%>\%.
  
  \item [Seasoning:] a 1 standard-deviation (about <%print(f'{data_scaler_dict[omap["PR"]]["sd"]["age"]:.0f}')%>
    months) increase in loan age causes the odds-ratio to increase by %
    <%print(f'{100*abs(np.exp(pr_hier_b_out.loc["age", "mean"])-1.0):.0f}')%>\%.
  
\end{description}

In Figure~\ref{fig:diff_claims}, we examine differences in the ``frailty''%
\footnote{``Frailty'' refers to the unobserved random-effect adjustments
to the intercept term for distress probabilities.}
and ``slope'' effect of claims on distress hazards across originators.

\begin{figure}[hbt!]
\caption{Differential impact of claims}
\label{fig:diff_claims}
\scalebox{1.0}{
<<diff_claims, echo=False>>=

random_coef_df = pd.concat([st_map("I")[0], st_map("II")[0]])
g_means = np.vstack((st_map("I")[1], st_map("II")[1]))

fig, ax = plt.subplots(1,2, figsize=(10, 5), gridspec_kw={'width_ratios': [2, 3]})
sns.scatterplot(data=random_coef_df, x="α", y="β", hue="originator", ax=ax[0])
sns.scatterplot(
    [g_means[0][0], g_means[1][0]], 
    [g_means[0][1], g_means[1][1]], 
    color="red", s=50, alpha=0.5, ax=ax[0]
)
sns.barplot(
    data=random_coef_df[["state", "originator", "β"]],
    x="state", y="β", hue="originator", ax=ax[1]
)
ax[1].set_xlabel("State")
_ = ax[1].set_xticklabels(ax[1].get_xticklabels(), rotation=45, size=7)
plt.tight_layout()
@
}
\end{figure}


\section{Distress}
In this section, we take a sample of 100 loans from the 2 originators we've studied in
this report and project the distress hazards forward until
the end of the third quarter. We do this by using the estimated hazard 
model and feed into it the claims forecast we generated in the previous
section. The claims forecast for each state is generated by taking the US
decay factor projection and applying it to the peak claims figure for the
state. The projections are depicted in Figure~\ref{fig:defer_proj}.

<<deferment_projection, echo=False>>=

def predict_hazard(originator):
    ''' generates hazard predictions '''

    horizon_date = datetime.date(2020, 9, 30)
    sub_df = make_df(hard_df_dict[originator], dep_var, ASOF_DATE, horizon_date)
    
    orig_model_key = ":".join([originator, "hier"])
    aaa, zzz, s_1_df = simulate(
      sub_df, dep_var, claims_dict["chg_df"], originator, ASOF_DATE, out_dict[orig_model_key]
    )

    return zzz
@

\begin{figure}[htb!]
\caption{Distress hazards}
\label{fig:defer_proj}
\scalebox{1}{
<<defer_proj, echo=False>>=
idx = pd.IndexSlice

lc_zzz = predict_hazard(omap["LC"])
lc_zzz_df = lc_zzz.groupby("stop").agg(
    y=(dep_var, np.mean), ymean=("ymean", np.mean),
    ystd=("ystd", np.mean), y5=("y5", np.mean), y95=("y95", np.mean)
).reset_index()

pr_zzz = predict_hazard(omap["PR"])
pr_zzz_df = pr_zzz.groupby("stop").agg(
    y=(dep_var, np.mean), ymean=("ymean", np.mean),
    ystd=("ystd", np.mean), y5=("y5", np.mean), y95=("y95", np.mean)
).reset_index()

fig, ax = plt.subplots(2,1, figsize=(10, 10))
for i in lc_zzz.index.get_level_values(0).to_series().sample(n=100, random_state=12345):
    lc_zzz.loc[idx[i, "2020-03-14":"2020-07-26"], ["ymean"]].reset_index().plot(
        x="edate", y="ymean", ax=ax[0], legend=False, alpha=0.25,
        title="Originator " + omap["LC"]
    )
ax[0].plot(lc_zzz_df["stop"], lc_zzz_df["ymean"], linewidth=50, color="red", label="Average")
_ = ax[0].set(xlabel='Week ending', ylabel='Hazard')

for i in pr_zzz.index.get_level_values(0).to_series().sample(n=100, random_state=12345):
    pr_zzz.loc[idx[i, "2020-03-14":"2020-07-26"], ["ymean"]].reset_index().plot(
        x="edate", y="ymean", ax=ax[1], legend=False, alpha=0.25,
        title="Originator " + omap["PR"]
    )
ax[1].plot(pr_zzz_df["stop"], pr_zzz_df["ymean"], linewidth=50, color="red", label="Average")
_ = ax[1].set(xlabel='Week ending', ylabel='Hazard')
plt.tight_layout()
@
}
\end{figure}

\section{Conclusion}

Our goals in this report were two-fold. First, we wanted to set out a
rigorous and transparent statistical framework for analyzing and
forecasting distress rates due to the COVID shock to the economy. 
While the data in the study are from
the consumer loan sector, the framework established here has
general applicability to a much broader range of assets. Second,
we wanted to establish quantitative bounds around what we should
expect for distress rates over the next quarter. An important
derivative work to this report is the development of a cashflow
engine and valuation model to incorporate these assessments into
loan pricing. A key element of that analysis will be a
determination of the ``cure'' rate from distress --- a complicated
excercse in its own right. For example, Groshen~\cite{groshencovid}
writes that
\begin{quote}
 \emph{
   \dots 83 percent of the increase in unemployment in March come[\emph{sic}]
 from workers on temporary furloughs, not permanent layoffs.
 }
\end{quote}
This may be significant in determining whether we are more
likely to have a speedier recovery rather than a more traditional, drawn out 
slog from a classical recession. We see hints of this in our data as well ---
for example, in the vast majority of cases, borrowers have cited ``Curtailment
of Income'' as the reason for seeking hardship deferments.
On the other hand, if the nature of the employment
shock turns into a more permanent layoffs instead of temporary furloughs, the outlook could be
more dire.

These aspects need to be explored in greater 
detail and we leave these considerations for another paper, preferring to 
deal with our problem in manageable bite-sized chunks. Regardless, the forecasts 
presented in this report should still serve to provide a lower bound on valuation 
if one is willing to assume that all borrowers granted deferrals are likely to 
simply go delinquent when their deferment term expires.

\clearpage
\section{Appendix}
\subsection{Model}

The modeling framework used in this report draws upon statistical
tools used in the analysis of events with a ``time-until'' component
to them. In our case, the time-until, or ``lifetime''  we are 
interested in predicting is the time until a borrower asks the
servicer for a deferment or goes delinquent. Time in this context is measured from
an assumed epoch start date of March 14th, 2020 which we assume
as the start of the COVID-19 crisis for our purposes and is the
same across all loans.

\subsection{Pooled}
We first describe the general setup of the model, assuming a 
``pooled'' setup where the concept of clusters and correlation
of outcomes for loans within clusters is initially ignored. This
allows us to provide a basic framework which is then extended
to the hierarchical setting. The exposition here is based on
and closely follows the excellent series of lecture notes by 
\cite{grodriguez}.

Let the time-to-event $T$ be a continuous variable with Cumulative
Distribution Function (CDF), $F(t)$ and density $f(t)$. $F(t)$
defines $P(T < t)$ or the cumulative probability that the
lifetime lasts until duration $t$. An alternative representation
more commonly employed in the survival literature is the
``Survival Function'' $S(t)$ which is the complement of the CDF:

\begin{equation}
S(t) = P(T >= t) = 1 - F(t) = \int_t^\infty \! f(x) \, \mathrm{d}x.
\end{equation}
The density function of lifetimes can be represented as $F'(t)$,
the derivative of the CDF.

An alternative characterization of the distribution of $T$
is given by the hazard function, or instantaneous rate of
occurrence of the event, defined as:
\begin{equation} \label{eq:hazard}
\lambda(t) = \lim_{\mathrm{d}t \to 0} \frac{P(t
    \leq T < t + \mathrm{d}t| T \geq t)}{\mathrm{d}t}
\end{equation}

The numerator in equation~\ref{eq:hazard} is the conditional
probability of an event ocurring in the interval $t, t + \mathrm{d}t)$,
conditional on it not having occured before $t$. The denominator
is the length of the time interval, which means the hazard
represents a rate of ocurrence. In the limit, as
$\mathrm{d}t$ goes to 0 --- the hazard represents the
instanteous rate of occurence of the event.

The conditional probability in the numerator may be written as the ratio
of the joint probability that $T$ is in the interval $[t, t + \mathrm{d}t)$ and $T \geq t$ (which
is, of course, the same as the probability that $t$ is in the interval), to the
probability that $T \geq t$. The former may be written as $f(t)\mathrm{d}t$ for
small $\mathrm{d}t$, while the latter is $S(t)$ by definition. Dividing by $\mathrm{d}t$ and passing
to the limit gives the useful result:
\begin{equation} \label{eq:haz_alt}
\lambda(t) = \frac{f(t)}{S(t)}
\end{equation}
Since $-f(t)$ is the derivative of $S(t)$, equation~\ref{eq:haz_alt} can also be written as:
\begin{equation}
\lambda(t) = -\frac{\mathrm{d}}{\mathrm{d}t} \ln(S(t))
\end{equation}
Integrating from 0 to $t$ and imposing the known boundary condition $S(0) = 1$
(since by construction, the event cannot have occurred at time 0), 
we obtain the following expression:
\begin{equation} \label{eq:cum_hazard}
S(t) = \exp\left\{-\int_{0}^{t}\lambda(x)\mathrm{d}x\right\}
\end{equation}

The integration term in equation~\ref{eq:cum_hazard} is referred to as the
\emph{cumulative hazard} and is denoted
\begin{equation}
\Lambda(t) = \int_{0}^{t}\lambda(x)\mathrm{d}x
\end{equation}

One can think of the cumulative hazard as the total risk of the event
happening from the time of entry into the state to time $t$.

In the simplest case, the hazard is constant over time spent in the state,
i.e., $\lambda(t) = \lambda$ for all $t$. The corresponding survival
function is $S(t) = \exp(-\lambda t)$ which is an exponential
distribution with parameter $\lambda$. The density is obtained as the
product of the hazard and the survivor function, or:
\begin{equation}
f(t) = \lambda \exp \left \{-\lambda t \right \}
\end{equation}
the mean of which is $1/\lambda$.

As an aside, a few interest points are worth noting here: 
\begin{itemize}
    \item There is a close connection between industry practice around the modeling
        of cash flows on loans and hazard rates. There are two operative concepts
        here - SMM and MDR which stand for Single Monthly Mortality and
        Monthly Default Rate respectively.
    \item Both represent unscheduled principal
        that is paid or written down, as a fraction of the balance outstanding.
        In this respect, the equivalence to the hazard function is clear -
        the unscheduled principal paid at time $t$ is the density $f(t)$ while
        the balance still outstanding is $S(t)$.
    \item Typically these concepts are
        applied to groups of loans and aggregations of principal paid and
        balance outstanding, but the analytical apparatus of hazard rates is
        directly application at each individual loan level.
\end{itemize}

In analyzing cash flows, we are interested in events that can
lead to a change in state and change the profile of scheduled
cashflows. Once a loan is created, its lifetime
(and/or its state) can change or be terminated through a multiplicity of ways.
These events can be modeled using \textbf{cause-specific hazard functions}.
The cause-specific hazard function for the $j$th failure type is:

\begin{equation} \label{eq:cause_specific_hazard}
\lambda_{j}(t) = \lim_{\mathrm{d}t \to 0} \frac{P(t \leq T < t +
    \mathrm{d}t, J = j| T \geq t)}{\mathrm{d}t}
\end{equation}
The total hazard of leaving the state can be written as the sum of
cause-specific hazards, $\lambda(t,x) = \sum_{j=1}^{J} \lambda_{j}(t,x)$.
Here, $x$ represents covariates that impact the cause-specific hazards
in a systematic way and allows us to write the hazard as:

\begin{equation}
\label{eqn:prop_hazard}
\lambda_{j}(t) = \lambda_{0,j} \exp \left \{x_{t}\beta \right \}
\end{equation}
where $\lambda_{0,j}$ represents an unspecified baseline hazard.

When durations are observed only over grouped intervals (we know an
event occurred between period $t_{k-1}$ and period $t_{k}$), then
the probability of observing an exit in the interval $[t_{k-1}, t_{k}]$
can be derived as follows. Since the failure in this interval is the
complement of surviving the interval, conditional on the observation
having survived until period $t_{k-1}$, it can written as:

\begin{eqnarray}\label{eq:grouped_hazard}
P(y_{j,t_{k}} = 1) & = & 1 - P(T > t_{k}|T \geq t_{k-1}) \\
               & = & 1 - \exp \left \{ -\exp(x_{t}'\beta) \int_{t_{k-1}}^{t_{k}}
               \lambda_{0,j}(\tau) \mathrm{d}\tau \right \}
\end{eqnarray}

Defining the rightmost term as a constant:
\begin{equation}
\alpha_{t_{k}} = \log\left \{ \int_{t_{k-1}}^{t_{k}}
    \lambda_{0,j}(\tau)\mathrm{d}\tau \right \}
\end{equation}
we can rewrite equation~\ref{eq:grouped_hazard} as follows:
\begin{equation}\label{eq:grouped_hazard_final}
P(y_{j,t_{k}} = 1) = 1 - \exp \left \{ -\exp(x_{t}'\beta + \alpha_{t_{k}} ) \right \}
\end{equation}

\begin{itemize}
    \item This is a binary dependent variable with a complementary log-log link
        and can be analyzed using standard discrete-choice models. The data is
        structured in the form of a panel for each borrower, contributing 1 record
        for every time unit that the loan is observed for.
    \item A status indicator
        represents whether the loan was still alive at the time of data
        collection or had an event ($y_{t_{k}} = 0,1$).
    \item The hazard function parameters can be estimated using either multinomial
        models or equivalently as a set of pairwise binary dependent models in a
        situation where multiple events are possible.
\end{itemize}

The likelihood function for this model can be derived as follows. Suppose we
have observations $t_{i}$ on $N$ lifetimes of interest (e.g., time to a deferral
request) and we also have, for each loan, an indicator variable $d_{i}$ which
is 1 if $t_{i}$ denotes an observed time to a deferral request, and 0 otherwise.
In the latter situation, the observation is considered ``censored''.

The likelihood for an observation that is known to have deferred at time $t_{i}$
can be written as the product of the survivor and hazard
functions:
\begin{equation}
L_i = f(t_i) = S(t_i) \lambda(t_i).
\end{equation}
If we consider a censored observation, we have:
\begin{equation}
L_i = S(t_i),
\end{equation}

Since we have both types of observations, we know all observations lived
at least until time $t_{i}$, but an observed deferral time with $d_{i} = 1$
implies we need to multiply the survivor funtion by the hazard at time $t_{i}$.
The combined likelihood across all observations is:
\begin{equation}
L = \prod_{i=1}^n L_i = \prod_i \lambda(t_i)^{d_i} S(t_i).
\end{equation}
Taking logs and simplifying, we have:
\begin{equation}
\log L = \sum_{i=1}^n \{ d_i \log \lambda(t_i) - \Lambda(t_i) \}.
\end{equation}

\begin{comment}
\subsubsection{Piecewise-exponential}
In our specific case, we know the exact day on which the
borrower's deferment request was approved, or they were declared
delinquent. Thus, we can take advantage
of this extra bit of information by treating time $t_{i}$ as continous,
but in a setup where the observed times are bucketed into discrete
intervals. This also allows us to accomodate covariates that may change 
over the course of time $t_{i}$ but are external to the process
and known at the start of each interval. Ths is referred to as Piece-Wise 
Exponential (PWE) for the underlying baseline hazard $\lambda_{t}$ in the literature. 

Under this specification, the time $t_{i}$ can be grouped into $J$ intervals, 
[$\tau_{0}, \tau_{1}, \tau_{j-1},\tau_{j}, \dots, \tau_{J}$].
If $t_{i} > \tau_{j}$, then the time spent in the interval is simply
$\tau_{j} - \tau_{j-1}$. If the individual experienced an event or was
censored in the interval $j$, then the time spent in that interval
is $t_{ij} = t_{i} - \tau_{j-1}$. For each of these intervals, we
can create a binary variable $d_{ij} = 0/1$ depending on whether
the loan survived that interval and went into the next interval.

The first term in the log-likelihood can now be written as:
\begin{equation}
d_i \log \lambda_i(t_i) = d_{ij(i)}\log\lambda_{ij(i)},
\end{equation}

The second term in the log-likelihood can be expressed as:
\begin{equation}
\Lambda_i(t_i) = \int_0^{t_i} \lambda_i(t)dt = \sum_{j=1}^{j(i)} t_{ij}\lambda_{ij},
\end{equation}
Here, we rely on the fact that the cumulative hazard integral is
composed of $j(i)$ terms and can be represented as the sum of these ``mini''
integrals. Each interval contributes the hazard $\lambda_{i}$ multiplied by
the time spent in the interval, which is 1 for all except the last where it
is $t_{ij}$. Since the $d_{ij}$ is zero for all intervals except the last,
we can consolidate both terms in the log-likelihood under one summation:
\begin{equation}
\log L_i = \sum_{j=1}^{j(i)} \{ d_{ij}\log\lambda_{ij} - t_{ij}\lambda_{ij}\}.
\end{equation}

The contribution of each interval, which is a sub-observation for each loan,
to the log-likelihood, is equivalent to the $d_{ij}$ being drawn from a
Poisson distribution with mean $\mu_{ij} = t_{ij}\lambda_{ij}$. The likelihood
for this would be written as:
\begin{equation}
\log L_{ij} = d_{ij}\log \mu_{ij} - \mu_{ij} =
d_{ij}\log(t_{ij}\lambda_{ij}) - t_{ij}\lambda_{ij}.
\end{equation}

If we substitute $t_{ij}\lambda_{ij}$ for $\mu_{ij}$, we get
\begin{equation}
\log L_{ij} = d_{ij}\log \mu_{ij} - \mu_{ij} =
d_{ij}\log(t_{ij}\lambda_{ij}) - t_{ij}\lambda_{ij}.
\end{equation}

This is the same as the previous formulation, with the exception of
the $d_{ij}\log(t_{ij})$ term --- this is only valid for the last
interval and so contributes $\log(t_{ij})$ to the log-likelihood.
This is equivalent to a Poisson model with an offset term equal
to $\log(t_{ij})$. This is the final model specification we use
in this report.

\end{comment}
 
\subsection{Hierarchical}
In order to extend this to a hierarchical bayesian framework, we make the
additional assumption that some of the terms in $\beta$ are cluster-specific. In
other words, the impact of this subset of variables on the distress hazard is
different for loans in different clusters. We treat each geographic region,
more specifically, a state, as the cluster. All loans from the same state
have a close relationship to each other, which may be the result of 
state-specific laws, regulations and customs. In the case of COVID-19,
given the state-specific nature of restrictions, the grouping is a natural
one to consider.

The schematic for the 
hierarchical model (incorporating additional tweaks to aid in the sampling)
is presented in Figure~\ref{fig:hierarchical_bayes_model_dag_pr}.
The ``hierarchical'' nature of the parameters is clearly
drawn as one moves from the top to the bottom of the
graph, with the final step being the simulation from
the posteriors using the observed value of the dichotomous
distress variable as the observed outcome.

\begin{figure}[hbt!]
\centering
\caption{Hierarchical bayes model}
\label{fig:hierarchical_bayes_model_dag_pr}
\scalebox{0.5}{
<<hierarchical_bayes_model_dag_pr, echo=False>>=
zzz = pm.model_to_graphviz(out_dict[omap["PR"] + ":hier"]["model"])
_ = zzz.render(
  directory="figures",
  filename="pr_hier_model", format="png",
  cleanup=True
)
@
}
\includegraphics[width=\linewidth]{figures/pr_hier_model.png}
\end{figure}

In our model, the impact of duration and the year-over-year percentage change in 
weekly initial claims, at the state level, are treated this way. Additionally, while
we make every attempt to control for \emph{observable} hetereogeneity across
loans, there may be unobserved factors that impact the distress hazard. We
model this as a ``frailty'' term that is state-specific, e.g., all loans within
a state share the same unobserved factor. 

In other words, the $x_{t}'\beta$ term in equation~\ref{eqn:prop_hazard} can be 
expanded and rewritten as:

\begin{equation}
\label{eqn:hierarchical}
\begin{array}{lcl}
  x_{t}'\beta & = & b x_{ij} + \eta_{j} z_{ij} + \gamma_{j}\\
  \eta_{j} & \sim & \mathcal{N}(\mu_{\eta}, \sigma_{\eta})\\
  \gamma_{j} & \sim & \mathcal{N}(\mu_{\gamma}, \sigma_{\gamma})
\end{array}
\end{equation}

By now, we have the basic outlines of the task at hand. We have a portfolio
of loans, and on some of these, borrowers have asked the servicer for a
deferral of payment. This is the result of an economic shock generated by 
the widespread shutdowns in response to the COVID-19 virus. We want to understand
how the probability of a borrower requesting a deferral depends on the attributes
of the borrower and the loan, as well as state level metrics of labor market distress.

The components of the $x$ and $z$ matrices are as follows:
\begin{itemize}
\item $x$: FICO, original balance, Debt-To-Income (DTI) ratio,
  stated monthly income, loan age. In addition, we have categorical 
  variables for loan credit grade, purpose, employment status, and
  original term.
\item $z$: Year-over-year percentage change in weekly claims (reported as of the Saturday
  of the week). Since the weekly intervals in which $t$ is observed coincides
  with the claims reporting period, we are assuming that these are observed 
  contemporaneously. In other word, the trigger for the decision to file
  a claim and ask for a deferment occurred sometime in the past, but both
  are recorded within the same week. The parameter measuring the impact
  of weekly claims for each loan is state-specific.
\end{itemize}

All right-hand-side variables are standardized by subtracting the
mean and dividing by the standard deviation. In addition, all the
categorical variables are encoded as dummmy variables with the first
level of the category being treated as the reference category. This
simplifies the interpretation of the model coefficients. The intercept
term for each state can then be used to compute the probability of
distress for a ``reference'' loan. 


In our case, for the first institution, this loan can 
be defined as a \textbf{Grade 1} loan, with the following additional characterstics:\,
\textbf{FICO:} <%print(f'{data_scaler_dict[omap["LC"]]["mu"]["fico"]:.2f}')%>, \,
\textbf{Original Balance:} \$<%print(f'{data_scaler_dict[omap["LC"]]["mu"]["original_balance"]:,.2f}')%>, \,
\textbf{DTI:} <%print(f'{(data_scaler_dict[omap["LC"]]["mu"]["dti"]):.2f}')%>\%, \,
\textbf{Monthly Income:} \$<%print(f'{data_scaler_dict[omap["LC"]]["mu"]["stated_monthly_income"]:,.2f}')%>, \,
\textbf{Age:} <%print(f'{data_scaler_dict[omap["LC"]]["mu"]["age"]:.2f}')%> months. 

For the second
institution, the corresponding figures are as follows: \,
\textbf{FICO:} <%print(f'{data_scaler_dict[omap["PR"]]["mu"]["fico"]:.2f}')%>, \,
\textbf{Original Balance:} \$<%print(f'{data_scaler_dict[omap["PR"]]["mu"]["original_balance"]:,.2f}')%>, \,
\textbf{DTI:} <%print(f'{(data_scaler_dict[omap["PR"]]["mu"]["dti"]):.2f}')%>\%, \,
\textbf{Monthly Income:} \$<%print(f'{data_scaler_dict[omap["PR"]]["mu"]["stated_monthly_income"]:,.2f}')%>, \,
\textbf{Age:} <%print(f'{data_scaler_dict[omap["PR"]]["mu"]["age"]:.2f}')%> months. 

It is further assumed
that the borrower used the loan proceeds for \textbf{consolidating debt},
that the borrower was either \textbf{employed} at the time of loan application
or employment history information was available at the application date, 
the \textbf{amortization term} of the loan was 3 years, and that the borrower 
\textbf{owned} their home. In effect, for a loan that matches these attributes, 
the $x$ term is a zero matrix, leaving the intercept as the only
term on the right-hand side. 

The variables in the $z$ matrix have a similar interpretation ---
they represent the deviation from the mean of the weekly claims ratio.

\subsection{Claims Sensitivity}

\begin{table}[htb]
\centering
\caption{Originator I: claims effects}
\label{tbl:lc_claims_table}
\scalebox{0.8}{
<<lc_claims_table, echo=False, results="tex">>=
df_η = lc_hier_st_out.loc[idx[:, "η"], :].droplevel(level=1)
cnames = ["mean", "sd", "hpd_3%", "hpd_97%", "r_hat"]
c_fmt = "".join(["r"] * (1 + df_η.shape[1]))
print(df_η.to_latex(index=True))
@
}
\end{table}

\begin{table}[htb]
\centering
\caption{Originator II: claims effects}
\label{tbl:pr_claims_table}
\scalebox{0.8}{
<<pr_claims_table, echo=False, results="tex">>=
df_η = pr_hier_st_out.loc[idx[:, "η"], :].droplevel(level=1)
print(df_η.to_latex(index=True))
@
}
\end{table}

\begin{table}[htb]
\centering
\caption{Originator I: covariate effects}
\label{tbl:lc_covariate_table}
\scalebox{1}{
<<lc_covariate_table, echo=False, results="tex">>=

print(lc_hier_b_out[["mean", "sd", "hpd_3%", "hpd_97%", "r_hat"]].to_latex(
  index=True, column_format="rrrrrr",
  )
)
@
}
\end{table}

\begin{table}[htb]
\centering
\caption{Originator II: covariate effects}
\label{tbl:pr_covariate_table}
\scalebox{1}{
<<pr_covariate_table, echo=False, results="tex">>=
print(pr_hier_b_out[["mean", "sd", "hpd_3%", "hpd_97%", "r_hat"]].to_latex(
  index=True, column_format="rrrrrr",
  )
)
@
}
\end{table}

\bibliographystyle{plainnat}
\bibliography{corona}

\end{document}
